{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "challenging-prairie",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "empty-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "complete-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('data/pickle_jar/cleaned.pkl', 'rb')\n",
    "tweets = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "tweets.reset_index(inplace=True)\n",
    "tweets.drop(columns = 'index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "shared-madrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1970258 entries, 0 to 1970257\n",
      "Data columns (total 21 columns):\n",
      " #   Column                   Dtype         \n",
      "---  ------                   -----         \n",
      " 0   created_at               datetime64[ns]\n",
      " 1   id                       int64         \n",
      " 2   full_text                object        \n",
      " 3   truncated                bool          \n",
      " 4   in_reply_to_screen_name  object        \n",
      " 5   is_quote_status          bool          \n",
      " 6   retweet_count            int64         \n",
      " 7   favorite_count           int64         \n",
      " 8   lang                     object        \n",
      " 9   retweeted_status         object        \n",
      " 10  names                    object        \n",
      " 11  screen_names             object        \n",
      " 12  locations                object        \n",
      " 13  follower_counts          int64         \n",
      " 14  user_created_at          object        \n",
      " 15  verified                 bool          \n",
      " 16  statuses_counts          int64         \n",
      " 17  location                 object        \n",
      " 18  country                  object        \n",
      " 19  hashtags                 object        \n",
      " 20  withheld_in_countries    object        \n",
      "dtypes: bool(3), datetime64[ns](1), int64(5), object(12)\n",
      "memory usage: 276.2+ MB\n"
     ]
    }
   ],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "approximate-cleanup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1970258, 21)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "interpreted-mitchell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at                       0\n",
       "id                               0\n",
       "full_text                        0\n",
       "truncated                        0\n",
       "in_reply_to_screen_name    1525932\n",
       "is_quote_status                  0\n",
       "retweet_count                    0\n",
       "favorite_count                   0\n",
       "lang                             0\n",
       "retweeted_status           1970258\n",
       "names                            0\n",
       "screen_names                     0\n",
       "locations                        0\n",
       "follower_counts                  0\n",
       "user_created_at                  0\n",
       "verified                         0\n",
       "statuses_counts                  0\n",
       "location                   1889581\n",
       "country                    1889581\n",
       "hashtags                   1234431\n",
       "withheld_in_countries      1970213\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "silver-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.095% of the dataset has location data\n"
     ]
    }
   ],
   "source": [
    "location_perc = 100 * tweets.dropna(subset =['location']).shape[0] / tweets.shape[0]\n",
    "print(f'{round(location_perc, 3)}% of the dataset has location data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "rubber-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweets.dropna(subset=['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "celtic-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_all = []\n",
    "for row in df['hashtags']:\n",
    "    for item in row:\n",
    "        hashtags_all.append(item.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "relevant-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hashtags = list(pd.Series(hashtags_all).value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "southeast-closure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['climatechange',\n",
       " 'globalwarming',\n",
       " 'climateaction',\n",
       " 'environment',\n",
       " 'climate',\n",
       " 'actonclimate',\n",
       " 'energy',\n",
       " 'climatechangeisreal',\n",
       " 'auspol',\n",
       " 'sustainability']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_hashtags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "egyptian-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "denier_tags = ['climatechangeisfalse', 'climatechangenotreal', 'climatechangehoax', \n",
    "               'globalwarminghoax', 'tcot', 'ccot', 'tlot', 'pjnet', 'rednationrising', 'votered', \n",
    "               'libtard', 'libtards', 'maga']\n",
    "\n",
    "believer_tags = ['climatechangeisreal', 'actonclimate', 'extinctionrebellion', 'climateemergency', \n",
    "                  'climateactionnow', 'capitalism', 'public_health', 'climateaction', 'humanityextinction',\n",
    "                'activism', 'noplanetb', 'savetheplanet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ultimate-vitamin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "believer = []\n",
    "denier = []\n",
    "unsure = []\n",
    "believe_series = []\n",
    "count = 0\n",
    "for idx, row in df['hashtags'].iteritems():\n",
    "    believe = 0\n",
    "    deny = 0 \n",
    "    for tag in row:\n",
    "        if tag.lower() in denier_tags:\n",
    "            deny += 1\n",
    "        elif tag.lower() in believer_tags:\n",
    "            believe += 1\n",
    "    if (believe > 0) and (deny == 0):\n",
    "        believer.append(int(idx))\n",
    "        believe_series.append(1)\n",
    "    elif (believe == 0) and (deny > 0):\n",
    "        denier.append(int(idx))\n",
    "        believe_series.append(0)\n",
    "    else:\n",
    "        unsure.append(int(idx))\n",
    "        believe_series.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "respective-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(believer = believe_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "demographic-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_train = df.dropna(subset=['believer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "painted-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_list = stopwords.words('english') + (list(string.punctuation) + \n",
    "                                               denier_tags + believer_tags + unique_hashtags[:100])\n",
    "\n",
    "def re_clean(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'[@][\\w]+','', tweet)\n",
    "    tweet = re.sub(r'[#]','', tweet)\n",
    "    tweet = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', tweet)\n",
    "    tweet = re.sub(r'\\s{2,15}', ' ', tweet)\n",
    "    tweet = re.sub(r'\\n', ' ', tweet)\n",
    "    tweet = re.sub(r'\\s\\s', ' ', tweet)\n",
    "    tweet = tweet.strip()\n",
    "    \n",
    "    tok = nltk.regexp_tokenize(tweet, r\"([a-zA-Z]+(?:'[a-z]+)?)\")\n",
    "    toks = [word.lower() for word in tok if word.lower() not in stopwords_list]\n",
    "    lemma = [lemmatizer.lemmatize(token) for token in toks]\n",
    "    \n",
    "    return ' '.join(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "incorporated-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: What is Sustainable Seafood? Learn more here: https://t.co/asJ9CxxnbI #climatechange #climateaction \n",
      "#environment \n",
      "#energy https://t.co/oO9ugGtoyz\n",
      "\n",
      "CLEANED: seafood learn\n"
     ]
    }
   ],
   "source": [
    "def check_cleaned(df, n=None):\n",
    "    if not n:\n",
    "        n = np.random.randint(0, len(df) - 1)\n",
    "    print('ORIGINAL:',df.iloc[n]['full_text'])\n",
    "    print('\\nCLEANED:', re_clean(df.iloc[n]['full_text']))\n",
    "    \n",
    "check_cleaned(to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "fitted-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_tweets = to_train['full_text'].apply(lambda x: re_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "upset-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_train['believer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "hairy-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_tweets(tweet):\n",
    "    tok = nltk.regexp_tokenize(tweet, r\"([a-zA-Z]+(?:'[a-z]+)?)\")\n",
    "    toks = [word.lower() for word in tok if word.lower() not in stopwords_list]\n",
    "    lemma = [lemmatizer.lemmatize(token) for token in toks]\n",
    "    \n",
    "    return ' '.join(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "liberal-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = cleaned_tweets.apply(lambda x: process_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "improving-joshua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'globalwarming firenado seems really need'"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_data.iloc[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-torture",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "hungarian-upgrade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65         latest article vanguard federal government's f...\n",
       "77         researcher projected share population exposed ...\n",
       "80         apple trillion behemoth planet paying price en...\n",
       "86                                                          \n",
       "112        america burn official attend denial conference...\n",
       "                                 ...                        \n",
       "1970127                      environmentaljustice greenpeace\n",
       "1970131               u president report initiated rest case\n",
       "1970135                   i'd even take mushy pea baked brit\n",
       "1970205    elon musk kinda dick prob v rich ppl amp robot...\n",
       "1970245    concentration dioxide atmosphere reached avera...\n",
       "Name: full_text, Length: 114971, dtype: object"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "dependent-pantyhose",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"noplanetb climatechangeisreal climateactionnow [elon musk is kinda a dick. prob. only v. rich ppl &amp;robots/slaves will get 2 colonise other planets. ignore rich orange fools w/bad combover &amp;rightwingers w/vested interests in fossil fuels. time's running out: listen to science]\""
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweets.iloc[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "ultimate-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(processed_tweets, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "dramatic-chicken",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241715            wea nao bae iu go go facing solomonislands\n",
       "326980                                 threaten availability\n",
       "1180529                    look forward seeing advance group\n",
       "391597     yet another disturbing one today's manorama pi...\n",
       "1281744                                 low technology learn\n",
       "                                 ...                        \n",
       "1033881    thanks faith courageousconversations series co...\n",
       "1499113                                 today large oilspill\n",
       "1238133    absence president willing lead important ever ...\n",
       "1550446                                          affect rest\n",
       "898337     global inextricably synergistically linked un'...\n",
       "Name: full_text, Length: 86228, dtype: object"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "controversial-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('vec', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier(class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "occasional-lease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vec', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', RandomForestClassifier(class_weight='balanced'))])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "plastic-classic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9277041366593606"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "infinite-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_data_train = vectorizer.fit_transform(X_train)\n",
    "tf_idf_data_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-communist",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "potential-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "surprising-gossip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced')"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.fit(tf_idf_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "alternative-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = rf_classifier.predict(tf_idf_data_train)\n",
    "y_pred_test = rf_classifier.predict(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "incorporated-gibson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9278433009776293"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "relative-coupon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1222,   680],\n",
       "       [ 1394, 25447]])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-float",
   "metadata": {},
   "source": [
    "## Apply Classifier to remainder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "massive-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_class = tweets[tweets['hashtags'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "broke-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test = to_class['full_text'].apply(lambda x: re_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "skilled-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: good question. I'm going with 'more scared of the energy lobby than the insurance lobby'. Which is why DSA candidates have a plan that fuses climate and economy - the Green New Deal. (which is mentioned here, to be fair!) https://t.co/WHDAfweaIB\n",
      "\n",
      "CLEANED: good question i'm going scared lobby insurance lobby dsa candidate plan fuse economy new deal mentioned fair\n"
     ]
    }
   ],
   "source": [
    "check_cleaned(to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "certified-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_class_pred = pipeline.predict(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "stupid-mounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/browz/opt/anaconda3/envs/ds01/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/browz/opt/anaconda3/envs/ds01/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "to_class['clean_text'] = processed_test\n",
    "to_class['believer_pred'] = un_class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "indonesian-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred(df, n=None):\n",
    "    if n == None:\n",
    "        n = np.random.randint(0,len(df)-1)\n",
    "    print(df.iloc[n]['clean_text'], '\\n', df.iloc[n]['believer_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "corrected-abuse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "following-extent",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "low >= high",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-371-d6274ebfef24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_class_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_class_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'believer_pred'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-352-b4ecea8bf991>\u001b[0m in \u001b[0;36mcheck_pred\u001b[0;34m(df, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'believer_pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: low >= high"
     ]
    }
   ],
   "source": [
    "to_class_clean = to_class[to_class['in_reply_to_screen_name'].isna()]\n",
    "\n",
    "check_pred(to_class_clean[to_class_clean['believer_pred'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-sharing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds01] *",
   "language": "python",
   "name": "conda-env-ds01-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "theoretical-robinson",
   "metadata": {},
   "source": [
    "# Import Packages\n",
    "\n",
    "Begin by downloading hydrated dataset to `/data` from:\n",
    "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/5QCCUU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decreased-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import twint\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "import bigjson\n",
    "import sqlite3\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-alert",
   "metadata": {},
   "source": [
    "### Run the code below to dehydrate the dataset and split into manageable blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stuffed-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cd data/split\n",
    "# ! twarc hydrate climate_id.txt.00 > hdrate.jsonl\n",
    "# ! split -l 200000 hdrate.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-heath",
   "metadata": {},
   "source": [
    "#### Move split files to folder `/data/split`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-brazil",
   "metadata": {},
   "source": [
    "### Define function for cleaning datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "excited-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    # remove retweets\n",
    "    tweets = tweets[tweets['retweeted_status'].isna()].copy()\n",
    "    \n",
    "    # take nested info and bring to own list for appending to dataframe\n",
    "    names = [key['name'] for key in tweets['user']]\n",
    "    screen_names = [key['screen_name'] for key in tweets['user']]\n",
    "    locations = [key['location'] for key in tweets['user']]\n",
    "    follower_counts = [key['followers_count'] for key in tweets['user']]\n",
    "    user_created_at = [key['created_at'] for key in tweets['user']]\n",
    "    verified = [key['verified'] for key in tweets['user']]\n",
    "    statuses_counts = [key['statuses_count'] for key in tweets['user']]\n",
    "    \n",
    "    location = [key['full_name'] if key else None for key in tweets['place']]\n",
    "    country = [key['country'] if key else None for key in tweets['place']]\n",
    "    \n",
    "    hashtag_mess = [row['hashtags'] for row in tweets['entities']]\n",
    "    hashtags = [[dct['text'] for dct in lst] if len(lst) > 0 else None for lst in hashtag_mess]\n",
    "    \n",
    "    add_cols = {'names': names, 'screen_names': screen_names, 'locations': locations, \n",
    "                'follower_counts': follower_counts, 'user_created_at': user_created_at, 'verified': verified,\n",
    "                'statuses_counts': statuses_counts, 'location': location, 'country': country, 'hashtags': hashtags}\n",
    "    \n",
    "    # select columns to be dropped\n",
    "    drop_cols = ['id_str', 'display_text_range', 'entities', 'source', 'in_reply_to_status_id', \n",
    "                 'in_reply_to_status_id_str','in_reply_to_user_id_str', 'user', 'geo', 'coordinates', \n",
    "                 'place', 'contributors', 'in_reply_to_user_id', 'quoted_status_id', 'quoted_status_id_str', \n",
    "                 'quoted_status_permalink', 'quoted_status', 'favorited', 'retweeted', 'possibly_sensitive', \n",
    "                 'extended_entities', 'retweeted_status']\n",
    "    \n",
    "    # select kept columns\n",
    "    cols = [x for x in tweets.columns if x not in drop_cols] + list(add_cols.keys())\n",
    "    \n",
    "    # add nested features directly to DataFrame \n",
    "    for key, value in add_cols.items():\n",
    "        tweets[key] = value\n",
    "    \n",
    "    # clean date formats\n",
    "    tweets['created_at'] = pd.to_datetime(tweets['created_at'].dt.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    # reformat date and leave as string\n",
    "    tweets['user_created_at'] = pd.to_datetime(tweets['user_created_at']).dt.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    return tweets[cols]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continuous-deposit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start xaa\n",
      "start xab\n",
      "start xac\n",
      "start xad\n",
      "start xae\n",
      "start xaf\n",
      "start xag\n",
      "start xah\n",
      "start xai\n",
      "start xaj\n",
      "start xak\n",
      "start xal\n",
      "start xam\n",
      "start xan\n",
      "start xao\n",
      "start xap\n",
      "start xaq\n",
      "start xar\n",
      "start xas\n",
      "start xat\n",
      "start xau\n",
      "start xav\n",
      "start xaw\n",
      "start xax\n",
      "start xay\n",
      "start xaz\n",
      "start xba\n",
      "start xbb\n",
      "start xbc\n",
      "start xbd\n",
      "start xbe\n",
      "start xbf\n",
      "start xbg\n",
      "start xbh\n",
      "start xbi\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.DataFrame()\n",
    "for letter in string.ascii_lowercase:\n",
    "    print(f\"start xa{letter}\")\n",
    "    tweets = pd.concat([tweets, clean_tweets(pd.read_json(f'data/split/xa{letter}', lines=True))])\n",
    "for letter in string.ascii_lowercase[:9]:\n",
    "    print(f\"start xb{letter}\")\n",
    "    tweets = pd.concat([tweets, clean_tweets(pd.read_json(f'data/split/xb{letter}', lines = True))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-mentor",
   "metadata": {},
   "source": [
    "### Save concatenated DataFrame for next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index of concatenated dataFrame\n",
    "tweets.reset_index(inplace=True)\n",
    "tweets.drop(columns = 'index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "saving-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('data/cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "automated-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open('data/pickle_jar/cleaned.pkl', 'wb')\n",
    "pickle.dump(tweets, pickle_out)\n",
    "pickle_out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "forbidden-central",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1970258, 21)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(sentence): \n",
    "  \n",
    "    # Instntiate SentimentIntensityAnalyzer object\n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "  \n",
    "    # store scoring data in dictionary  \n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence) \n",
    "    \n",
    "    return sentiment_dict['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['vader'] = tweets['tweet'].map(lambda x: sentiment_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_score = tweets.vader.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Between {s_date} and {e_date}, the sentiment score for {key_word} was on average {round(sent_score, 3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-massage",
   "metadata": {},
   "source": [
    "## Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement function to get ticker symbol from company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-adapter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msft.quarterly_rev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"0x1a8E53C684f38E1AC640f3f510B0CbA3aFd3EE70\" == \"0x1a8E53C684f38E1AC640f3f510B0CbA3aFd3EE70\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds01] *",
   "language": "python",
   "name": "conda-env-ds01-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

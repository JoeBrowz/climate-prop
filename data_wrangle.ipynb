{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "theoretical-robinson",
   "metadata": {},
   "source": [
    "# Import Packages\n",
    "\n",
    "Begin by downloading hydrated dataset to `/data` from:\n",
    "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/5QCCUU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decreased-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import bigjson\n",
    "import string\n",
    "import pickle\n",
    "import twint\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-alert",
   "metadata": {},
   "source": [
    "### Run the code below to dehydrate the dataset and split into manageable blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stuffed-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cd data/split\n",
    "# ! twarc hydrate climate_id.txt.00 > hdrate.jsonl\n",
    "# ! split -l 200000 hdrate.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-heath",
   "metadata": {},
   "source": [
    "#### Move split files to folder `/data/split`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-brazil",
   "metadata": {},
   "source": [
    "### Define function for cleaning datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "excited-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    # remove retweets\n",
    "    tweets = tweets[tweets['retweeted_status'].isna()].copy()\n",
    "    \n",
    "    # take nested info and bring to own list for appending to dataframe\n",
    "    names = [key['name'] for key in tweets['user']]\n",
    "    screen_names = [key['screen_name'] for key in tweets['user']]\n",
    "    locations = [key['location'] for key in tweets['user']]\n",
    "    follower_counts = [key['followers_count'] for key in tweets['user']]\n",
    "    user_created_at = [key['created_at'] for key in tweets['user']]\n",
    "    verified = [key['verified'] for key in tweets['user']]\n",
    "    statuses_counts = [key['statuses_count'] for key in tweets['user']]\n",
    "    \n",
    "    location = [key['full_name'] if key else None for key in tweets['place']]\n",
    "    country = [key['country'] if key else None for key in tweets['place']]\n",
    "    \n",
    "    hashtag_mess = [row['hashtags'] for row in tweets['entities']]\n",
    "    hashtags = [[dct['text'] for dct in lst] if len(lst) > 0 else None for lst in hashtag_mess]\n",
    "    \n",
    "    add_cols = {'names': names, 'screen_names': screen_names, 'locations': locations, \n",
    "                'follower_counts': follower_counts, 'user_created_at': user_created_at, 'verified': verified,\n",
    "                'statuses_counts': statuses_counts, 'location': location, 'country': country, 'hashtags': hashtags}\n",
    "    \n",
    "    # select columns to be dropped\n",
    "    drop_cols = ['id_str', 'display_text_range', 'entities', 'source', 'in_reply_to_status_id', \n",
    "                 'in_reply_to_status_id_str','in_reply_to_user_id_str', 'user', 'geo', 'coordinates', \n",
    "                 'place', 'contributors', 'in_reply_to_user_id', 'quoted_status_id', 'quoted_status_id_str', \n",
    "                 'quoted_status_permalink', 'quoted_status', 'favorited', 'retweeted', 'possibly_sensitive', \n",
    "                 'extended_entities', 'retweeted_status']\n",
    "    \n",
    "    # select kept columns\n",
    "    cols = [x for x in tweets.columns if x not in drop_cols] + list(add_cols.keys())\n",
    "    \n",
    "    # add nested features directly to DataFrame \n",
    "    for key, value in add_cols.items():\n",
    "        tweets[key] = value\n",
    "    \n",
    "    # clean date formats\n",
    "    tweets['created_at'] = pd.to_datetime(tweets['created_at'].dt.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    # reformat date and leave as string\n",
    "    tweets['user_created_at'] = pd.to_datetime(tweets['user_created_at']).dt.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    return tweets[cols]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-colleague",
   "metadata": {},
   "source": [
    "# Run function to import data \n",
    "Loop through each split, dehydrated file and concatenate to single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continuous-deposit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start xaa\n",
      "start xab\n",
      "start xac\n",
      "start xad\n",
      "start xae\n",
      "start xaf\n",
      "start xag\n",
      "start xah\n",
      "start xai\n",
      "start xaj\n",
      "start xak\n",
      "start xal\n",
      "start xam\n",
      "start xan\n",
      "start xao\n",
      "start xap\n",
      "start xaq\n",
      "start xar\n",
      "start xas\n",
      "start xat\n",
      "start xau\n",
      "start xav\n",
      "start xaw\n",
      "start xax\n",
      "start xay\n",
      "start xaz\n",
      "start xba\n",
      "start xbb\n",
      "start xbc\n",
      "start xbd\n",
      "start xbe\n",
      "start xbf\n",
      "start xbg\n",
      "start xbh\n",
      "start xbi\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.DataFrame()\n",
    "for letter in string.ascii_lowercase:\n",
    "    # to track progress\n",
    "    print(f\"start xa{letter}\")\n",
    "    # concatenate to existing df \n",
    "    tweets = pd.concat([tweets, clean_tweets(pd.read_json(f'data/split/xa{letter}', lines=True))])\n",
    "\n",
    "# second half of files\n",
    "for letter in string.ascii_lowercase[:9]:\n",
    "    # to track progress\n",
    "    print(f\"start xb{letter}\")\n",
    "    # concatenate to existing df \n",
    "    tweets = pd.concat([tweets, clean_tweets(pd.read_json(f'data/split/xb{letter}', lines = True))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-mentor",
   "metadata": {},
   "source": [
    "### Save concatenated DataFrame for next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index of concatenated dataFrame\n",
    "tweets.reset_index(inplace=True)\n",
    "tweets.drop(columns = 'index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "saving-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "tweets.to_csv('data/cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "automated-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle directly\n",
    "pickle_out = open('data/pickle_jar/cleaned.pkl', 'wb')\n",
    "pickle.dump(tweets, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-species",
   "metadata": {},
   "source": [
    "# Supplement Dataset with more Denial Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "alleged-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_scrape(key_word, start, end, tweets_per):\n",
    "        c = twint.Config()\n",
    "        c.Search = key_word\n",
    "        c.Limit = tweets_per\n",
    "        c.Lang = 'en'\n",
    "        c.Since = start\n",
    "        c.Until = end\n",
    "        c.Pandas = True\n",
    "        c.Hide_output = True\n",
    "        c.Count = True\n",
    "        c.Replies = True\n",
    "        c.Filter_retweets = True\n",
    "        \n",
    "        twint.run.Search(c)\n",
    "        \n",
    "        cols = ['date', 'username', 'name', 'tweet', 'retweet', 'hashtags', 'nlikes', 'search']\n",
    "        \n",
    "        df = twint.storage.panda.Tweets_df\n",
    "        \n",
    "        return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hidden-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 2 Tweets.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 1 Tweets.\n",
      "[+] Finished: Successfully collected 9017 Tweets.\n",
      "[+] Finished: Successfully collected 9017 Tweets.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 13240 Tweets.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[+] Finished: Successfully collected 1011 Tweets.\n"
     ]
    }
   ],
   "source": [
    "denier_tags = ['climatechangeisfalse', 'climatechangenotreal', 'climatechangehoax', \n",
    "               'globalwarminghoax']\n",
    "\n",
    "denier_sup = pd.DataFrame() \n",
    "for tag in denier_tags:\n",
    "    denier_sup = pd.concat([denier_sup, tweet_scrape(tag, '2019-06-01', '2021-02-10', 20000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-wednesday",
   "metadata": {},
   "source": [
    "## Pickle for next notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "optional-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "denier_sup.reset_index(inplace=True)\n",
    "# clear extra index column\n",
    "denier_sup.drop(columns = 'index', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "short-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open('data/pickle_jar/denier_sup.pkl', 'wb')\n",
    "pickle.dump(denier_sup, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-transmission",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Get a glimpse at data to inform how modeling process will go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds01] *",
   "language": "python",
   "name": "conda-env-ds01-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
